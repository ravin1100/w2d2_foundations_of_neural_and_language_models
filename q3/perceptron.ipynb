{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Perceptron From Scratch\n",
        "\n",
        "In this notebook, we'll implement a single-neuron logistic regression model (perceptron) to classify fruits (apples vs bananas) based on their features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fruit dataset\n",
        "df = pd.read_csv('fruit.csv')\n",
        "print(df.head())\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for class balance\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(df['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the data\n",
        "X = df[['length_cm', 'weight_g', 'yellow_score']].values\n",
        "y = df['label'].values\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Original features (first 3 rows):\")\n",
        "print(X[:3])\n",
        "print(\"\\nScaled features (first 3 rows):\")\n",
        "print(X_scaled[:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticNeuron:\n",
        "    def __init__(self, n_features):\n",
        "        # Initialize weights and bias randomly\n",
        "        self.weights = np.random.randn(n_features) * 0.01\n",
        "        self.bias = np.random.randn() * 0.01\n",
        "        \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        return self.sigmoid(z)\n",
        "    \n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        \"\"\"Binary cross-entropy loss\"\"\"\n",
        "        epsilon = 1e-15  # Small value to avoid log(0)\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predictions to avoid numerical issues\n",
        "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    \n",
        "    def compute_gradients(self, X, y_true, y_pred):\n",
        "        \"\"\"Compute gradients for weights and bias\"\"\"\n",
        "        m = X.shape[0]\n",
        "        dw = (1/m) * np.dot(X.T, (y_pred - y_true))\n",
        "        db = (1/m) * np.sum(y_pred - y_true)\n",
        "        return dw, db\n",
        "    \n",
        "    def update_parameters(self, dw, db, learning_rate):\n",
        "        \"\"\"Update weights and bias using gradients\"\"\"\n",
        "        self.weights -= learning_rate * dw\n",
        "        self.bias -= learning_rate * db\n",
        "    \n",
        "    def train(self, X, y, learning_rate=0.1, epochs=1000, early_stop_loss=0.05):\n",
        "        \"\"\"Train the model using batch gradient descent\"\"\"\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward(X)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(y_pred, y)\n",
        "            losses.append(loss)\n",
        "            \n",
        "            # Compute accuracy\n",
        "            y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "            accuracy = np.mean(y_pred_binary == y)\n",
        "            accuracies.append(accuracy)\n",
        "            \n",
        "            # Compute gradients\n",
        "            dw, db = self.compute_gradients(X, y, y_pred)\n",
        "            \n",
        "            # Update parameters\n",
        "            self.update_parameters(dw, db, learning_rate)\n",
        "            \n",
        "            # Print progress every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "            \n",
        "            # Early stopping if loss is below threshold\n",
        "            if loss < early_stop_loss:\n",
        "                print(f\"Early stopping at epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "                break\n",
        "        \n",
        "        return losses, accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "np.random.seed(42)  # For reproducibility\n",
        "model = LogisticNeuron(n_features=X_scaled.shape[1])\n",
        "\n",
        "# Initial predictions before training\n",
        "initial_predictions = model.forward(X_scaled)\n",
        "initial_pred_binary = (initial_predictions >= 0.5).astype(int)\n",
        "initial_accuracy = np.mean(initial_pred_binary == y)\n",
        "initial_loss = model.compute_loss(initial_predictions, y)\n",
        "\n",
        "print(f\"Initial random model - Loss: {initial_loss:.4f}, Accuracy: {initial_accuracy:.4f}\")\n",
        "print(\"Initial weights:\", model.weights)\n",
        "print(\"Initial bias:\", model.bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "early_stop_loss = 0.05\n",
        "\n",
        "losses, accuracies = model.train(X_scaled, y, learning_rate, epochs, early_stop_loss)\n",
        "\n",
        "# Final predictions after training\n",
        "final_predictions = model.forward(X_scaled)\n",
        "final_pred_binary = (final_predictions >= 0.5).astype(int)\n",
        "final_accuracy = np.mean(final_pred_binary == y)\n",
        "final_loss = model.compute_loss(final_predictions, y)\n",
        "\n",
        "print(f\"\\nFinal model - Loss: {final_loss:.4f}, Accuracy: {final_accuracy:.4f}\")\n",
        "print(\"Final weights:\", model.weights)\n",
        "print(\"Final bias:\", model.bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the training progress\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses)\n",
        "plt.title('Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accuracies)\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different learning rates\n",
        "learning_rates = [0.01, 0.1, 0.5, 1.0]\n",
        "epochs = 500\n",
        "results = []\n",
        "\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    # Reset model with same initialization\n",
        "    np.random.seed(42)\n",
        "    model_lr = LogisticNeuron(n_features=X_scaled.shape[1])\n",
        "    \n",
        "    # Train with this learning rate\n",
        "    losses_lr, accuracies_lr = model_lr.train(X_scaled, y, learning_rate=lr, epochs=epochs, early_stop_loss=0.001)\n",
        "    results.append((lr, losses_lr, accuracies_lr))\n",
        "    \n",
        "    # Plot loss\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.plot(losses_lr)\n",
        "    plt.title(f'Loss with Learning Rate = {lr}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Binary Cross-Entropy Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot all learning rates on the same graph for comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "for lr, losses_lr, _ in results:\n",
        "    plt.plot(losses_lr[:100], label=f'LR = {lr}')  # Show first 100 epochs for clarity\n",
        "\n",
        "plt.title('Loss Comparison for Different Learning Rates')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the decision boundary\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# We'll use length and yellow_score as our 2D features for visualization\n",
        "feature1_idx = 0  # length_cm\n",
        "feature3_idx = 2  # yellow_score\n",
        "\n",
        "# Create a mesh grid\n",
        "x_min, x_max = X_scaled[:, feature1_idx].min() - 1, X_scaled[:, feature1_idx].max() + 1\n",
        "y_min, y_max = X_scaled[:, feature3_idx].min() - 1, X_scaled[:, feature3_idx].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                     np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "# For each point in the mesh, calculate the prediction\n",
        "Z = np.zeros((xx.shape[0], xx.shape[1]))\n",
        "for i in range(xx.shape[0]):\n",
        "    for j in range(xx.shape[1]):\n",
        "        # Create a feature vector with the average weight_g (feature2) value\n",
        "        avg_feature2 = np.mean(X_scaled[:, 1])\n",
        "        features = np.array([xx[i, j], avg_feature2, yy[i, j]])\n",
        "        Z[i, j] = model.forward(features)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdBu)\n",
        "plt.colorbar()\n",
        "\n",
        "# Plot the training points\n",
        "scatter = plt.scatter(X_scaled[:, feature1_idx], X_scaled[:, feature3_idx], \n",
        "                     c=y, edgecolors='k', cmap=plt.cm.RdBu)\n",
        "plt.xlabel('Standardized Length (cm)')\n",
        "plt.ylabel('Standardized Yellow Score')\n",
        "plt.title('Decision Boundary')\n",
        "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
